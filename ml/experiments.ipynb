{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiversx_sdk_wallet import Mnemonic\n",
    "from multiversx_sdk_core import Address\n",
    "from multiversx_sdk_core import AddressFactory\n",
    "\n",
    "mnemonic = Mnemonic.generate()\n",
    "words = mnemonic.get_words()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_key = mnemonic.derive_key(0)\n",
    "public_key = secret_key.generate_public_key()\n",
    "\n",
    "print(secret_key.hex())\n",
    "print(\"Public key (hex-encoded):\", secret_key.hex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = AddressFactory(\"erd\")\n",
    "address = factory.create_from_hex(\"19b9cf83c67a1c21ed313ac4d75e66df71d267a15dcf609f37ca5695ee86bb7f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = supabase.table('chatbot_messages') \\\n",
    "  .insert({\n",
    "    \"user_id\": \"123456789\",\n",
    "    \"chat_id\": \"chat_001\",\n",
    "    \"text\": \"Hello, how can I help you?\",\n",
    "    \"is_bot\": True\n",
    "  }) \\\n",
    "  .execute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone\n",
    "repo_path = \"./github-scrapper/code_scrapped\"\n",
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".rs\"],\n",
    "    parser=LanguageParser(language=Language.RUST, parser_threshold=500)\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)\n",
    "rust_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.RUST, \n",
    "                                                               chunk_size=2000, \n",
    "                                                               chunk_overlap=200)\n",
    "texts = rust_splitter.split_documents(documents)\n",
    "len(texts)\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\") \n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
    "\n",
    "question = \"Generate multiversx contract code: Create a contract that holds the number 18 and provides a function to increase it.\"\n",
    "result = qa(question)\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rust_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.RUST, \n",
    "                                                               chunk_size=2000, \n",
    "                                                               chunk_overlap=200)\n",
    "texts = rust_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\") \n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Generate multiversx contract code: Create a contract that holds the number 18 and provides a function to increase it.\"\n",
    "result = qa(question)\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "base_url = \"https://docs.multiversx.com/\"\n",
    "\n",
    "def get_links_from_page(soup, base_url):\n",
    "    # Extract all links on the current page that are relative and prepend with base URL\n",
    "    links = [base_url + a['href'] for a in soup.find_all('a', href=True) \n",
    "             if a['href'].startswith('/') and 'multiversx' in base_url.lower()]\n",
    "    return links\n",
    "\n",
    "\n",
    "url = \"https://docs.multiversx.com/\"\n",
    "all_contents = []\n",
    "visited_urls = set()\n",
    "to_visit_urls = [url]\n",
    "\n",
    "while to_visit_urls:\n",
    "    current_url = to_visit_urls.pop()\n",
    "    if current_url in visited_urls:\n",
    "        continue\n",
    "\n",
    "    response = requests.get(current_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Store content\n",
    "    all_contents.append(soup.prettify())\n",
    "\n",
    "    # Get all links from the current page\n",
    "    links = get_links_from_page(soup, base_url)\n",
    "    to_visit_urls.extend(links)\n",
    "    print(to_visit_urls)\n",
    "    visited_urls.add(current_url)\n",
    "\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def get_links_from_page(soup, base_url):\n",
    "    # Extract all relative links on the current page and convert to absolute URL\n",
    "    links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)\n",
    "             if a['href'].startswith('/')]\n",
    "    return links\n",
    "\n",
    "def recursive_scrape(url, visited_urls=None):\n",
    "    if visited_urls is None:\n",
    "        visited_urls = set()\n",
    "\n",
    "    if url in visited_urls:\n",
    "        return visited_urls\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return visited_urls  # If we don't get a successful response, return visited URLs\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    # Get all relative links from the current page\n",
    "    links = get_links_from_page(soup, url)\n",
    "    print(visited_urls)\n",
    "    for link in links:\n",
    "        if link not in visited_urls:\n",
    "            visited_urls.update(recursive_scrape(link, visited_urls))\n",
    "    return visited_urls\n",
    "\n",
    "start_url = \"https://docs.multiversx.com/\"\n",
    "all_visited_links = recursive_scrape(start_url)\n",
    "print(all_visited_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links_from_page(soup, base_url):\n",
    "    # Extract all relative links on the current page and convert to absolute URL\n",
    "    links = [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True)\n",
    "             if a['href'].startswith('/')]\n",
    "    return links\n",
    "\n",
    "def get_text_from_page(soup):\n",
    "    # Get textual content from the page\n",
    "    return \" \".join([t.get_text() for t in soup.find_all(True) if t.name not in ['script', 'style']])\n",
    "\n",
    "def recursive_scrape(url, visited_urls=None, content_list=None):\n",
    "    if visited_urls is None:\n",
    "        visited_urls = set()\n",
    "    if content_list is None:\n",
    "        content_list = []\n",
    "\n",
    "    if url in visited_urls:\n",
    "        return content_list\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return content_list  # If we don't get a successful response, return the collected content so far\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    # Get the textual content of the current page\n",
    "    page_content = get_text_from_page(soup)\n",
    "    content_list.append(page_content)\n",
    "\n",
    "    # Get all relative links from the current page\n",
    "    links = get_links_from_page(soup, url)\n",
    "\n",
    "    print(visited_urls)\n",
    "    for link in links:\n",
    "        if link not in visited_urls:\n",
    "            content_list = recursive_scrape(link, visited_urls, content_list)\n",
    "\n",
    "    return content_list\n",
    "\n",
    "start_url = \"https://docs.multiversx.com/\"\n",
    "all_content = recursive_scrape(start_url)\n",
    "\n",
    "# Printing all the content collected\n",
    "for content in all_content:\n",
    "    print(content)\n",
    "\n",
    "\n",
    "filename = \"scraped_content.txt\"\n",
    "\n",
    "# Write the collected content to the file\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    for content in all_content:\n",
    "        f.write(content + \"\\n\\n\")\n",
    "        print(content)  # If you still want to print the content to the console\n",
    "\n",
    "print(f\"Content has been saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "loader = TextLoader(\"./scraped_v2.txt\")\n",
    "documents = loader.load()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"commanderastern\"\n",
    "dataset_path = f\"hub://{username}/qanda_mx2\"\n",
    "db = DeepLake(\n",
    "    dataset_path=dataset_path, embedding_function =embeddings, overwrite=True\n",
    ")\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAIChat\n",
    "\n",
    "db = DeepLake(\n",
    "    dataset_path='./qanda_mx2/', embedding_function =embeddings, read_only=True\n",
    ")\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAIChat(model=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"With what macro is a module trait defined\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
